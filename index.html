<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Soumya Banerjee</title>
  
  <meta name="author" content="Soumya Banerjee">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <!--   <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon">
 -->
 <script type="text/javascript" src="scripts/hidebib.js"></script>

 <!-- Scramble Script by Jeff Donahue -->
 <script src="scripts/scramble.js"></script>

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Soumya Banerjee</name>
              </p>
              <p>
                Hii! I am a doctoral student with <a href="https://vinaypn.github.io/">Prof. Vinay P. Namboodiri</a> and <a href="https://www.cse.iitk.ac.in/users/piyush/">Prof. Piyush Rai</a> in <a href="https://www.cse.iitk.ac.in/">Dept. Of Computer Science & Engineering</a> at IIT Kanpur. I also closely collaborate with <a href="https://scholar.google.com/citations?user=7x6GZ1EAAAAJ&hl=en">Dr. Vinay Kumar Verma</a>. I work in the area of general machine learning, deep learning, active learning and computer vision.
              </p>

              <p>
                
                Prior to joining IIT Kanpur (reverse chronologically), I was a Master's student in <a href="https://bhu.ac.in/Site/UnitHomeTemplate/2_8_1146_Faculty-of-Science-Computer-Science">Dept. Of Computer Science</a> at <a href="https://bhu.ac.in/Site/Home/1_2_16_Main-Site">BHU</a> and a Bachelor's student in Dept. Of Computer Science at <a href="https://vidyamandira.ac.in/">Ramakrishna Mission Vidyamandira</a>. 
              </p>
              
              <p style="text-align:center">
                <a href="mailto:soumyab@cse.iitk.ac.in">Email</a> &nbsp/&nbsp
                <a href="data/SoumyaBanerjee-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/SoumyaBanerjee-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=p7BoMpMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/SamaelBanerjee">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/SouBanerjee">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/soumya-banerjee-81050a145/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/SoumyaBanerjee.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/SoumyaBanerjee_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <hr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

          <tbody><tr><td>
            <heading>&nbsp;&nbsp;News</heading>

            <!--sectionheading>&nbsp;&nbsp;News</sectionheading-->
            
            <ul>
                <li> Our work on 'Streaming LifeLong Learning With Any-Time Inference' has been accepted at <a href="https://www.icra2023.org/" >ICRA2023.</a></li>
                <!--li> Invited talk on "Diaster Management using CV" at Digital University Kerala.</a></li>
                <li> Joined as an assistant professor in DSE, IISER Bhopal</a></li>
                <li> Received  "Outstanding PhD Thesis Award" from IIT Kanpur.</a></li>
                <li> Our work on 'Bias-free learning' has been accepted at <a href="https://aaai.org/Conferences/AAAI-22/" >AAAI-22.</a></li>
                <li> Our work on 'Context learning-based 3D face generation' has been accepted at <a href="https://wacv2022.thecvf.com/" >WACV-22.</a></li>
                <li> <a href="https://www.journals.elsevier.com/image-and-vision-computing" >ICCV 2021 </a>Doctoral Consortium.</li-->
              

              <!--a href="javascript:toggleblock(&#39;news&#39;)">---- show more ----</a-->

                <!--div id="news" style="display:none">
                    <li> 'Uncertainty based VQG (MUMC)' has been accepted at <a href="https://iccv2021.thecvf.com/home" >Image Vision Computing</a> Journal - Elsevier.</li>
                    <li> 'Dropout Domain Adaptation' has been accepted at <a href="https://www.journals.elsevier.com/neurocomputing" >NeuroComputing</a> Journal - Elsevier .</li>
                    <li> Join as a Post-Doc Research Fellow at <a href="https://www.kuleuven.be/english/">KU Leuven </a> Belgium. </li>
                    <li> 'Informative Discriminator in DA' has been accepted at  <a href="https://www.journals.elsevier.com/image-and-vision-computing" >Image Vision Computing</a> Journal - Elsevier.</li>
                    <li> Two papers on 'Fingerprint enchnacement and domain adaptation' have been accepted at <a href="https://www.ijcnn.org/" >IJCNN-21</a> .</li>
                    <li> Our 'Multimodal Audio-Video generation' paper got accepted at <a href="https://2021.ieeeicassp.org" >ICASSP-21</a> .</li>
                    <li> Defended PhD thesis on  "Understading Transfer Learning between Domains and Tasks".</li>
                    <li> Paper accepted at workshop of <a href="http://vcmi.inesctec.pt/xai4biom_wacv/index.html
                ">xAI4Biometrics</a> at WACV-21.</li>
                    <li> Two papers are accepted at <a href="http://wacv2021.thecvf.com/"> WACV-21</a>.</li>
                    <li> Selected as a DAAD <a href="https://www.daad.de/en/the-daad/postdocnet/fellows/fellows/#Kurmi"> AI Post-Doc Net Fellow </a>. </li>
                    <li> Join as a Post-Doc at <a href="https://cvit.iiit.ac.in/">CVIT </a> Hyderabad. </li>
                    <li> Paper accepted in <a href="https://www.journals.elsevier.com/neurocomputing"> Neurocomputing </a>. </li>
                    <li> Delivered Open Seminar on "Understading Transfer Learning between Domains and Tasks"</li>
                    <li>Two papers accepted in  Workshop on Closing the Loop Between Vision and Language<a href="https://sites.google.com/site/iccv19clvllsmdc/program"> CLVL</a> at ICCV-19</li>
                    <li>Paper accepted in <a href="http://wacv2020.thecvf.com/"> WACV-20</a></li>

                    <li>Particiapted in  Amazon Research Day-2019 at Bangalore</li>

                    <li>Paper accepted in <a href="https://bmvc2019.org/"> BMVC-19</a></li>

                    <li>Presented our work at  <a href="https://ieeexplore.ieee.org/document/8492375"> IJCNN</a> Hungary.</li>

                    <li>Presented our work at  <a href="https://cvpr2019.thecvf.com/"> CVPR</a> Long Beach, US.</li>
                    <li> Delivered talk on "Baysian Domain Adaptation" at <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists/"> Qualcomm Bangalore</a> </li>
                    <li>Named <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists">Qualcomm Innovation Fellowship 2019</a> Finalists.</li>

                    <li> Paper accepted in <a href="https://ieeexplore.ieee.org/document/8492375"> IJCNN-19</a></li>

                    <li> Paper accepted in <a href="https://cvpr2019.thecvf.com/"> CVPR-19</a></li>

                    <li> Particiapted in  Amazon Research Day-2018 at Bangalore</li>


                </div-->
            </ul>
          </td></tr>
        </tbody></table>

        <hr>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>

              <p>
                My current research aims to enable lifelong learning in the deep neural networks while trained incrementally with the sequentially coming data possibly from an unbounded stream with the following constraints: (i) the neural network can have access to the current data only, and access to the previously observed data is forbidden, and (ii) the model should be able to adapt to the changes in the data distribution without suffering from catastrophic forgetting. The following publication reflects my current research interest. Please check my <a href="https://scholar.google.com/citations?user=p7BoMpMAAAAJ&hl=en">Google Scholar</a> page for the full list.
              </p>

              <!-- The following is a list of publications that best reflect my current research interests. Please check my Google scholar page for the full list. -->

              <!--p>
              	Most of my published papers focus on using concepts like self-supervision and try to exploit the natural correlation present between speech and lips. The image enhancement works that I did before joining IIITH did not also require manual labeling. <b>Authors marked with a * have an equal contribution in the research done.</b>   
              </p-->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            <!-- StarttPaper one -->
            <!--tr>
              <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://SouBanerjee.github.io/"><img src="images/bayesian_streaming_learning.png" alt="sym" width="90%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>



                  <td width="67%" valign="top" style="padding-top:10px">
                <p><a href="https://SouBanerjee.github.io/" id="interspeech22_id">
                  <img src="images/news.png"  width="6%" style="border-style: none">
                <heading>Not So Generalized Keyword Spotting using ASR embeddings</heading></a><br>
                Kirandevraj R, <strong>Vinod K Kurmi</strong>, Vinay P Namboodiri, C V Jawahar<br>
                <em>Conference of the International Speech Communication Association (Interspeech) </em> 2022, Incheon Korea
                </p>
                <div class="paper" id="interspeech22">
                <a href="javascript:toggleblock(&#39;interspeech22_abs&#39;)">abstract</a> |
                <a shape="rect" href="javascript:togglebib(&#39;interspeech22&#39;)" class="togglebib">bibtex</a> |
                paper (coming soon) |

                <p align="justify"> <i id="interspeech22_abs" style="display: none;">Keyword Spotting (KWS) detects a set of pre-defined spoken keywords. Building a KWS system for an arbitrary set re- quires massive training datasets. We propose to use the text transcripts from an Automatic Speech Recognition (ASR) sys- tem alongside triplets for KWS training. The intermediate rep- resentation from the ASR system trained on a speech corpus is used as acoustic word embeddings for keywords. Triplet loss is added to the Connectionist Temporal Classification (CTC) loss in the ASR while training. This method achieves an Average Precision (AP) of 0.843 over 344 words unseen by the model trained on the TIMIT dataset. In contrast, the Multi-View re- current method that learns jointly on the text and acoustic em- beddings achieves only 0.218 for out-of-vocabulary words. This method is also applied to low-resource languages such as Tamil by converting Tamil characters to English using transliteration. This is a very challenging novel task for which we provide a dataset of transcripts for the keywords. Despite our model not generalizing well, we achieve a benchmark AP of 0.321 on over 38 words unseen by the model on the MSWC Tamil keyword set. The model also produces an accuracy of 96.2% for classifi- cation tasks on the Google Speech Commands dataset. </i></p>

                <pre xml:space="preserve" style="display: none;">@inproceedings{kiran_inter22,
                Author = {R,Kiran.
                and Kurmi, Vinod K
                and Namboodiri, Vinay P
                and Jawhar, CV},
                Title = {Generalized Keyword
                Spotting using ASR embeddings},
                Booktitle = {InterSpeech},
                Year   = {2022}
                }
                </pre>
                </div>
              </td>
            </tr-->

          <!-- End paper  one -->

          <!-- StarttPaper one -->

          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:0px;width:35%;vertical-align:middle">
              <div class="one">
                <img src="images/bayesian_streaming_learning.png" width="270">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://SouBanerjee.github.io/">
                <img src="images/news.png"  width="6%" style="border-style: none">
                <papertitle>Streaming LifeLong Learning With Any-Time Inference</papertitle>
              </a>
              <br>

			        <strong>Soumya Banerjee</strong>, 
              <a href="https://scholar.google.com/citations?user=7x6GZ1EAAAAJ&hl=en">Vinay Kumar Varma</a>,
              <a href="https://vinaypn.github.io/">Vinay Namboodiri</a><br>
              
              <em>IEEE International Conference on Robotics and Automation (ICRA) 2023</em>, 
              <br> ExCeL London, UK <br><br>

              <div class="paper" id="icra23">
                <a href="javascript:toggleblock(&#39;icra23_abs&#39;)">abstract</a> |
                <!--a shape="rect" href="javascript:togglebib(&#39;icra23&#39;)" class="togglebib"-->bibtex (coming soon)<!--/a--> |
                paper (coming soon) | arXiv (coming soon)


                <p align="justify" id="icra23_abs" style="display: none;"> Despite rapid advancements in the lifelong learning (LLL) research, a large body of research mainly focuses on improving the performance in the existing <i>static</i> continual learning (CL) setups. These methods lack the ability to succeed in a rapidly changing <i>dynamic</i> environment, where an AI agent needs to quickly learn new instances in a `single pass' from the non-i.i.d (also possibly temporally contiguous/coherent) data streams without suffering from catastrophic forgetting. For practical applicability, we propose a novel lifelong learning approach, which is streaming, i.e., a single input sample arrive in each time step, single pass, class-incremental, and subject to be evaluated at any moment. To address this challenging setup and various evaluation protocols, we propose a Bayesian framework, that enables fast parameter update, given a single training example, and enables any-time inference. We additionally propose an implicit regularizer in the form of snap-shot self-distillation, which effectively minimizes the forgetting further. We further propose an effective method that efficiently selects a subset of samples for online memory rehearsal and employs a new replay buffer management scheme that significantly boosts the overall performance. Our empirical evaluations and ablations demonstrate that the proposed method outperforms the prior works by large margins.</p>

                <!--pre xml:space="preserve" style="display: none;">@inproceedings{kiran_inter22,
                Author = {R,Kiran.
                and Kurmi, Vinod K
                and Namboodiri, Vinay P
                and Jawhar, CV},
                Title = {Generalized Keyword
                Spotting using ASR embeddings},
                Booktitle = {InterSpeech},
                Year   = {2022}
                }
                </pre-->
              </div>

              
            </td>
          </tr>

          <!-- End paper  one -->

          <!--tr onmouseout="nerd_stop()" onmouseover="nerd_start()", bgcolor="#ffffd0">
            <td style="padding:0px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/wav2lip.gif' width="270">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413532">
                <papertitle>A Lip Sync Expert Is All You Need For Speech To Lip Generation In The Wild</papertitle>
              </a>
              <br>

              <a href="https://scholar.google.co.in/citations?user=C-wGb2sAAAAJ&hl=en">K R Prajwal*</a>, 
              <strong>Rudrabha Mukhopadhyay*</strong>,
              <a href="https://vinaypn.github.io/">Vinay Namboodiri</a>,
              <a href="http://cvit.iiit.ac.in/people/faculty/cvit-faculty/jawahar">C.V. Jawahar</a>,
              <br>
              <em>ACM Multimedia, 2020 (Oral)</em>
              <br>
              <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/">Project Page</a> /
              <a href="https://youtu.be/0fXaDCZNOJc">Demo Video</a> /
              <a href="https://github.com/Rudrabha/Wav2Lip">Code</a> /
              <a href="http://bhaasha.iiit.ac.in/lipsync/">Interactive Demo</a> /
              <a href="https://arxiv.org/abs/2008.10010">arXiv</a> /
              <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413532">ACM DL</a>
              <p></p>
              <p>
              <b> Abstract: </b> In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. 
              </p>
              <p style="color:green;">
              	<b> Please feel free to contact me for possible commercial usage of this project! We are in the process of filing a patent and also licinsing our model (we have a much better one than the one released).</b>
              </p>
            </td>
          </tr>

          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:0px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/lip2wav.gif' width="270">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Prajwal_Learning_Individual_Speaking_Styles_for_Accurate_Lip_to_Speech_Synthesis_CVPR_2020_paper.pdf">
                <papertitle>Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis</papertitle>
              </a>
              <br>

              <a href="https://scholar.google.co.in/citations?user=C-wGb2sAAAAJ&hl=en">K R Prajwal*</a>, 
              <strong>Rudrabha Mukhopadhyay*</strong>,
              <a href="https://vinaypn.github.io/">Vinay Namboodiri</a>,
              <a href="http://cvit.iiit.ac.in/people/faculty/cvit-faculty/jawahar">C.V. Jawahar</a>,
              <br>
              <em>CVPR, 2020</em>
              <br>
              <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/speaking-by-observing-lip-movements">Project Page</a> /
              <a href="https://www.youtube.com/watch?v=HziA-jmlk_4">Demo Video</a> /
              <a href="https://github.com/Rudrabha/Lip2Wav">Code</a> /
              <a href="https://arxiv.org/abs/2005.08209">arXiv</a> /
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Prajwal_Learning_Individual_Speaking_Styles_for_Accurate_Lip_to_Speech_Synthesis_CVPR_2020_paper.pdf">CVF</a>
              <p></p>
              <p>
              <b> Abstract: </b> Humans involuntarily tend to infer parts of the conversation from lip movements when the speech is absent or corrupted by external noise. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate natural speech given only the lip movements of a speaker. Acknowledging the importance of contextual and speaker-specific cues for accurate lip-reading, we take a different path from existing works. We focus on learning accurate lip sequences to speech mappings for individual speakers in unconstrained, large vocabulary settings. To this end, we collect and release a large-scale benchmark dataset, the first of its kind, specifically to train and evaluate the single-speaker lip to speech task in natural settings. We propose a novel approach with key design choices to achieve accurate, natural lip to speech synthesis in such unconstrained scenarios for the first time. Extensive evaluation using quantitative, qualitative metrics and human evaluation shows that our method is four times more intelligible than previous works in this space.  
              </p>
            </td>
          </tr>


          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:0px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/TTS.png' width="270">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/text-to-speech-dataset-for-indian-languages">
                <papertitle>
                	IndicSpeech: Text-to-Speech Corpus for Indian Languages
 				</papertitle>
              </a>
              <br>

			  <a href="https://blogs.iiit.ac.in/monthly_news/nimisha-srivastava/">Nimisha Srivastava</a>, 
              <strong>Rudrabha Mukhopadhyay*</strong>,
              <a href="https://scholar.google.co.in/citations?user=C-wGb2sAAAAJ&hl=en">K R Prajwal*</a>, 
              <a href="http://cvit.iiit.ac.in/people/faculty/cvit-faculty/jawahar">C.V. Jawahar</a>,
              <br>
              <em>LREC, 2020</em>
              <br>
              <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/text-to-speech-dataset-for-indian-languages">Project Page</a> /
              <a href="https://forms.gle/p4RUG4E5FcyLXYiH9">Dataset</a> /
              <a href="https://www.aclweb.org/anthology/2020.lrec-1.789/">Paper</a>
              <p></p>
              <p>
              <b> Abstract: </b> India is a country where several tens of languages are spoken by over a billion strong population. Text-to-speech systems for such languages will thus be extremely beneficial for wide-spread content creation and accessibility. Despite this, the current TTS systems for even the most popular Indian languages fall short of the contemporary state-of-the-art systems for English, Chinese, etc. We believe that one of the major reasons for this is the lack of large, publicly available text-to-speech corpora in these languages that are suitable for training neural text-to-speech systems. To mitigate this, we release a 24 hour text-to-speech corpus for 3 major Indian languages namely Hindi, Malayalam and Bengali. In this work, we also train a state-of-the-art TTS system for each of these languages and report their performances. The collected corpus, code, and trained models are made publicly available. 
              </p>
            </td>
          </tr>

          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:0px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/lipgan.gif' width="270">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/10.1145/3343031.3351066">
                <papertitle>Towards Automatic Face-to-Face Translation</papertitle>
              </a>
              <br>

              <a href="https://scholar.google.co.in/citations?user=C-wGb2sAAAAJ&hl=en">K R Prajwal*</a>, 
              <strong>Rudrabha Mukhopadhyay*</strong>,
              <a href="https://jerinphilip.github.io/"> Jerin Philip </a>, 
              <a href="https://abskjha.github.io/"> Abhishek Jha </a>, 
              <a href="https://vinaypn.github.io/">Vinay Namboodiri</a>, 
              <a href="http://cvit.iiit.ac.in/people/faculty/cvit-faculty/jawahar">C.V. Jawahar</a>,
              <br>
              <em>ACM Multimedia, 2019 (Oral)</em>
              <br>
              <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/facetoface-translation">Project Page</a> /
              <a href="https://www.youtube.com/watch?v=aHG6Oei8jF0&list=LL2W0lqk_iPaqSlgPZ9GNv6w">Demo Video</a> /
              <a href="https://github.com/Rudrabha/LipGAN">Code</a> /
              <a href="https://arxiv.org/abs/2003.00418">arXiv</a> /
              <a href="https://dl.acm.org/doi/10.1145/3343031.3351066">ACM DL</a>
              <p></p>
              <p>
              <b> Abstract: </b> In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as "Face-to-Face Translation". As today's digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact on multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards "Face-to-Face Translation" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. 
              </p>
            </td>
          </tr>


          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:0px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/irgun.png' width="270">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/w13/html/Sharma_IRGUN_Improved_Residue_CVPR_2018_paper.html">
                <papertitle>
                	IRGUN: Improved Residue Based Gradual Up-Scaling Network for Single Image Super Resolution
 				</papertitle>
              </a>
              <br>

			  <a href="https://scholar.google.co.in/citations?user=deabq2AAAAAJ&hl=en">Manoj Sharma</a>, 
              <strong>Rudrabha Mukhopadhyay</strong>,
              <a href=#>Avinash Upadhyay</a>, 
              <a href="https://scholar.google.co.in/citations?user=POiv5fIAAAAJ&hl=en">Sriharsha Koundinya</a>,
              <a href="https://www.linkedin.com/in/ankit-shukla-43237657/?originalSubdomain=in">Ankit Shukla</a>,
              <a href="http://iitj.ac.in/institute/index.php?id=director">Santanu Chaudhury</a>,
              <br>
              <em>CVPR Workshops, 2018</em>
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w13/Sharma_IRGUN_Improved_Residue_CVPR_2018_paper.pdf">Paper</a> /
              <a href="https://github.com/Rudrabha/8X-Super-Resolution">Code</a> 
              <p></p>
              <p>
              <b> Abstract: </b> Convolutional neural network based architectures have achieved decent perceptual quality super resolution on natural images for small scaling factors (2X and 4X). However, image super-resolution for large magnification factors (8X) is an extremely challenging problem for the computer vision community. In this paper, we propose a novel Improved Residual based Gradual Up-Scaling Network (IRGUN) to improve the quality of the super-resolved image for a large magnification factor. IRGUN has a Gradual Upsampling and Residue-based Enhancment Network (GUREN) which comprises of series of Up-scaling and Enhancement blocks (UEB) connected end-to-end and fine-tuned together to give a gradual magnification and enhancement. Due to the perceptual importance of the luminance in super-resolution, the model is trained on luminance(Y) channel of the YCbCr image. Whereas, the chrominance components (Cb and Cr) channel is up-scaled using bicubic interpolation and combined with super-resolved Y channel of the image which is then converted to RGB. A cascaded 3D-RED architecture trained on RGB images is utilized to incorporate its inter-channel correlation. In addition to this, the training methodology is also presented in the paper. In the training procedure, the weights of the previous UEB are used in the next immediate UEB for faster and better convergence. Each UEB is trained on its respective scale by taking the output image of the previous UEB as input and corresponding HR image of the same scale as ground truth to the successive UEB. All the UEBs are then connected end-to-end and fine tuned. The IRGUN recovers fine details effectively at large (8X) magnification factors. The efficiency of IRGUN is presented on various benchmark datasets and at different magnification scales. 
              </p>
            </td>
          </tr>


          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:0px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/hyperspectral.png' width="270">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/w13/html/Koundinya_2D-3D_CNN_Based_CVPR_2018_paper.html">
                <papertitle>
                	2D-3D CNN Based Architectures for Spectral Reconstruction From RGB Images
 				</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.co.in/citations?user=POiv5fIAAAAJ&hl=en">Sriharsha Koundinya</a>,
              <a href="https://scholar.google.com/citations?user=jvXy1cgAAAAJ&hl=en">Himanshu Sharma</a>,
			  <a href="https://scholar.google.co.in/citations?user=deabq2AAAAAJ&hl=en">Manoj Sharma</a>, 
              <a href=#>Avinash Upadhyay</a>, 
              <a href=#>Raunak Manekar</a>, 
              <strong>Rudrabha Mukhopadhyay</strong>,
              
              <a href="https://www.ceeri.res.in/profiles/abhijit-karmakar/">Abhijit Karmakar</a>,
              <a href="http://iitj.ac.in/institute/index.php?id=director">Santanu Chaudhury</a>,
              <br>
              <em>CVPR Workshops, 2018</em>
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w13/Koundinya_2D-3D_CNN_Based_CVPR_2018_paper.pdf">Paper</a> 
              <p></p>
              <p>
              <b> Abstract: </b> Hyperspectral cameras are used to preserve fine spectral details of scenes that are not captured by traditional RGB cameras, due to the gross quantization of radiance in RGB images. Spectral details provide additional information that improves the performance of numerous image based analytic applications, but due to high hyperspectral hardware cost and associated physical constraints, hyperspectral images are not easily available for further processing. Motivated by the success of deep learning for various computer vision applications, we propose a 2D convolution neural network and a 3D convolution neural network based approaches for hyper-spectral image reconstruction from RGB images. A 2D-CNN model primarily focuses on extracting spectral data by considering only spatial correlation of the channels in the image, while in 3D-CNN model the inter-channel co-relation is also exploited to refine the extraction of spectral data. Our 3D-CNN based architecture achieves state-of-the-art performance in terms of MRAE and RMSE. In contrast of 3D-CNN, our 2D-CNN based architecture also achieves performance near by state-of-the-art with very less computational complexity.
              </p>
            </td>
          </tr>

          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:0px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/sr_inpainting.png' width="270">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-981-13-0020-2_18">
                <papertitle>
                	An End-to-End Deep Learning Framework for Super-Resolution Based Inpainting
 				</papertitle>
              </a>
              <br>
			  <a href="https://scholar.google.co.in/citations?user=deabq2AAAAAJ&hl=en">Manoj Sharma</a>, 
              <strong>Rudrabha Mukhopadhyay</strong>,
              <a href="http://iitj.ac.in/institute/index.php?id=director">Santanu Chaudhury</a>,
              <a href="https://web.iitd.ac.in/~brejesh/">Brejesh Lall</a>,

              <br>
              <em>NCVPRIPG, 2017</em>
              <br>
              <a href="https://link.springer.com/content/pdf/10.1007%2F978-981-13-0020-2_18.pdf">Paper</a> 
              <p></p>
              <p>
              <b> Abstract: </b> Image inpainting is an extremely challenging and open problem for the computer vision community. Motivated by the recent advancement in deep learning algorithms for computer vision applications, we propose a new end-to-end deep learning based framework for image inpainting. Firstly, the images are down-sampled as it reduces the targeted area of inpainting therefore enabling better filling of the target region. A down-sampled image is inpainted using a trained deep convolutional auto-encoder (CAE). A coupled deep convolutional auto-encoder (CDCA) is also trained for natural image super resolution. The pre-trained weights from both of these networks serve as initial weights to an end-to-end framework during the fine tuning phase. Hence, the network is jointly optimized for both the aforementioned tasks while maintaining the local structure/information. We tested this proposed framework with various existing image inpainting datasets and it outperforms existing natural image blind inpainting algorithms. Our proposed framework also works well to get noise resilient super-resolution after fine-tuning on noise-free super-resolution dataset. It provides more visually plausible and better resultant image in comparison of other conventional and state-of-the-art noise-resilient super-resolution algorithms.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Public Demos</heading>
              <p>
              	I have also presented our works in the form of demonstrations, at various conferences. Our demos has been well received and instrumental in increasing the visibility of the papers beyond the conferences we published them.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/eccv_demo.jpg' width="250">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/interplay-of-speech-and-lips">
                <papertitle>The Interplay of Speech and Lip Movements</papertitle>
              </a>
              <br>

              <a href="https://scholar.google.co.in/citations?user=C-wGb2sAAAAJ&hl=en">K R Prajwal*</a>, 
              <strong>Rudrabha Mukhopadhyay*</strong>,
              <a href="https://vinaypn.github.io/">Vinay Namboodiri</a>,
              <a href="http://cvit.iiit.ac.in/people/faculty/cvit-faculty/jawahar">C.V. Jawahar</a>,
              <br>
              <em>ECCV Demonstrations, 2020</em>
              <br>
              <a href="https://sites.google.com/view/interplay-of-speech-and-lips">Website</a> /
              <a href="https://www.youtube.com/watch?v=SnLonyyykeY&feature=youtu.be">Demo Video</a> /
              <a href="data/eccv_demo.pdf">Abstract</a>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/icpr_demo.png' width="250">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/interplay-of-speech-and-lips">
                <papertitle>The Interplay of Speech and Lip Movements</papertitle>
              </a>
              <br>

              <strong>Rudrabha Mukhopadhyay*</strong>,
              <a href="https://scholar.google.co.in/citations?user=C-wGb2sAAAAJ&hl=en">K R Prajwal*</a>, 
              <a href="https://scholar.google.co.in/citations?user=cD8J2-kAAAAJ&hl=en">Sindhu Hegde*</a>, 
              <a href="https://vinaypn.github.io/">Vinay Namboodiri</a>,
              <a href="http://cvit.iiit.ac.in/people/faculty/cvit-faculty/jawahar">C.V. Jawahar</a>,
              <br>
              <em>ICPR Demonstrations, 2020</em>
              <br>
              <a href="https://sites.google.com/view/interplay-of-speech-and-lips">Website</a> /
              <a href='https://youtu.be/ydj4Ach3d8I'>Demo Video</a> /
              <a href="data/icpr_demo.pdf">Demo Writeup</a>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/lip2wav_vi.png' width="250">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=#>
                <papertitle>Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis</papertitle>
              </a>
              <br>

              <strong>Rudrabha Mukhopadhyay*</strong>,
              <a href="https://scholar.google.co.in/citations?user=C-wGb2sAAAAJ&hl=en">K R Prajwal*</a>, 
              
              <a href="https://vinaypn.github.io/">Vinay Namboodiri</a>,
              <a href="http://cvit.iiit.ac.in/people/faculty/cvit-faculty/jawahar">C.V. Jawahar</a>,
              <br>
              <em>ICVGIP Vision-India Session, 2020</em>
              <br>
              <a href='https://youtu.be/vtDAjX-x6nE'>Video</a> /
              <a href='https://iiitaphyd-my.sharepoint.com/:p:/g/personal/radrabha_m_research_iiit_ac_in/EaoUpHFy65RLqRKqDzBEvzcBZBWa4v2VJDYqhmBM4plJ3Q?e=M3BoUO'>Slides</a>
              <p></p>
            </td>
          </tr>


          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/f2f_vi.png' width="250">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/drive/folders/18C2po2vVAMSyfOeGYCUJhFuIPVO9J5lb?usp=sharing">
                <papertitle>Towards Automatic Face-to-Face Translation</papertitle>
              </a>
              <br>

              <strong>Rudrabha Mukhopadhyay*</strong>,
              <a href="https://scholar.google.co.in/citations?user=C-wGb2sAAAAJ&hl=en">K R Prajwal*</a>, 
              
              <a href="https://vinaypn.github.io/">Vinay Namboodiri</a>,
              <a href="http://cvit.iiit.ac.in/people/faculty/cvit-faculty/jawahar">C.V. Jawahar</a>,
              <br>
              <em>NCVPRIPG Vision-India Session, 2019</em>
              <br>
              <a href="https://drive.google.com/drive/folders/18C2po2vVAMSyfOeGYCUJhFuIPVO9J5lb?usp=sharing">Videos & Materials</a> /
              <a href="data/f2f.pdf">Slides</a>
              <p></p>
            </td>
          </tr-->

        </tbody></table>

        <!--table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Grants</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/google.png' width="250">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            	<papertitle>Google Travel Grant</papertitle>
            	<p>
            		I received a grant to travel to Nice, France and attend ACM Multimedia, 2019. I had an oral presentation in the conference for our paper titled, "Towards Automatic Face-to-Face Translation". 
            	</p>
            </td>
          </tr>



        </tbody></table-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Peer-reviewing</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/reviewing.jpeg' width="250">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              I have reviewed for the following venues: 
              <ul>
                <li><strong>2023</strong>: CVPR</li>
                <li><strong>2022</strong>: NeurIPS, WACV, ICVGIP, Pattern Recognition</li>
              </ul>
            	<!--papertitle>Reviewer in WACV 2021</papertitle>
            	<p>
            		I acted as a reviewer in WACV, 2021 and reviewed two papers for the conference organizers.  
            	</p-->
            </td>
          </tr>


          <!--tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/ada.jpg' width="250">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Student System Administrator</papertitle>
              <p>
                I am a student system administrator for the HPC cluster of IIIT-Hyderabad. Our cluster is known as ADA and contains 252 GPUs and 2520 CPUs. I currently manage the shared resources needed to be allocated to around 400 users in the cluster. I was instrumental in acquiring new storage devices for the cluster and the subsequent shifting of large amounts of data. I am also a part of the high-level policymaking group overseeing the general operations of the cluster. 
              </p>
            </td>
          </tr>

          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <div class="one">
                <img src='images/blood_donor.png' width="230">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Voluntary Blood Donation</papertitle>
              <p>
                I have regularly donated blood since July, 2014 after I turned 18. According to the American Red Cross, one donation can save as many as three lives. I am truly committed to this cause and hope to continue the practice in future. 
              </p>
            </td>
          </tr-->

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
          <tbody><tr><td><br><p align="right"><font size="1.5">
          Modified version of template from <a href="http://www.cs.berkeley.edu/~barron/">this</a>, <a href="https://www.cs.cmu.edu/~dpathak/">this,</a> and <a href="http://jeffdonahue.com/">this</a>
          </font></p></td></tr>
      </tbody></table>


      </td>
    </tr>
  </table>
</body>

</html>
